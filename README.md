## Mini RAG

Ce projet pr√©sente un syst√®me RAG (Retrieval-Augmented Generation) complet permettant d'interroger un document PDF via un chatbot intelligent.
Il combine LangChain, FAISS, BGE embeddings, reranking et une g√©n√©ration locale via Ollama.

### Fonctionnalit√©s

- Chargement automatique d‚Äôun PDF (ex : Syntec Conseil ‚Äì M√©tiers de la Data)

- D√©coupage intelligent du texte en chunks

- Vectorisation avec le mod√®le BAAI/bge-m3

- Recherche s√©mantique via FAISS

- Reranking des passages avec BAAI/bge-reranker-v2-m3

- G√©n√©ration locale avec mistral:instruct (Ollama)

- Interface Streamlit simple pour poser des questions

- Pipeline optimis√© : MMR + Reranker + Prompt anti-hallucination

### Structure du projet :

MiniRAG/

‚îÇ‚îÄ‚îÄ My_Sreamlit_app.py           # Interface Streamlit

‚îÇ‚îÄ‚îÄ MiniRAG_Djebril_LAOUEDJ.ipynb # Notebook du projet

‚îÇ‚îÄ‚îÄ requirements.txt             # Versions exactes test√©es

‚îÇ‚îÄ‚îÄ README.md # Documentation (English)

‚îÇ‚îÄ‚îÄ README_FR.md # Documentation (French)

‚îÇ‚îÄ‚îÄ .gitignore                   # Fichiers ignor√©s

‚îî‚îÄ‚îÄ data                        # PDF d'exemple : Syntec-Conseil_Glossaire-des-principaux-m√©tiers-de-la-Data mais vous pouvez mettre celui que vous voulez.

### Exemple de pipeline RAG

Voici la logique g√©n√©rale du projet :

- Charger le PDF

- Chunking du texte

- G√©n√©ration des embeddings (BGE-M3)

- Indexation FAISS

- Retrieval (MMR)

- Reranking (BGE-Reranker)

- G√©n√©ration via Mistral:instruct (Ollama)

### Installation

1Ô∏è‚É£ Cloner le projet

git clone https://github.com/djbrl-laouedj/MiniRAG.git

cd MiniRAG

2Ô∏è‚É£ Installer les d√©pendances

‚ö†Ô∏è Les versions sont exactes pour √©viter les conflits LangChain. Merci d'utiliser requirements.txt.

pip install -r requirements.txt

3Ô∏è‚É£ Installer Ollama (si pas d√©j√† install√©)

‚û°Ô∏è https://ollama.com/download

Puis t√©l√©charger le mod√®le utilis√© :

ollama pull mistral:instruct

‚ñ∂Ô∏è Lancer l'application Streamlit

streamlit run app.py

L‚Äôinterface s‚Äôouvre automatiquement dans le navigateur.

### Sur Google Colab :

T√©l√©chargez tous les fichiers.

- Configuration ngrok

Si vous voulez exposer votre interface Streamlit :

Cr√©er un compte : https://ngrok.com

R√©cup√®rez votre cl√© : https://dashboard.ngrok.com/get-started/your-authtoken

Puis ajoutez-cela √† la fin d evotre code :

from pyngrok import ngrok
ngrok.set_auth_token("<VotreCl√©>")

!streamlit run My_Sreamlit_app.py &>/dev/null &

public_url = ngrok.connect(8501)
public_url

Et ce code pour relancer le streamlit :

// stop tous les tunnels ngrok pour repartir propre
from pyngrok import ngrok
try:
    ngrok.kill()
except:
    pass

### Voil√† pour finir un mini user-guide :

1. Uploader un PDF :

<img width="230" height="174" alt="image" src="https://github.com/user-attachments/assets/68d63482-dab0-4fd7-9da6-eb2c532127a1" />

2. Puis lancer la pipeline :

<img width="228" height="73" alt="image" src="https://github.com/user-attachments/assets/1283f0f6-4c97-4e82-8e59-24a076c70c15" />

3. Une fois fini, poser une question (exemple : Quelle est le r√¥le d'un Data Engineer)

<img width="1599" height="809" alt="image" src="https://github.com/user-attachments/assets/ecf844b9-4725-402d-8bd0-124febe23a01" />

4. R√©sultat (les sources peuvent √™tre vues ou non)

<img width="1550" height="694" alt="image" src="https://github.com/user-attachments/assets/ba5e04cc-467a-40ae-8506-52ae47a5799f" />

5. Malheureusement, si la question pos√© ne peut pas √™tre r√©pondu (par manque d'informations dans les donn√©es jointes), le chatbot retournera cela :

<img width="1028" height="154" alt="image" src="https://github.com/user-attachments/assets/fb838479-3c1a-4307-97ba-a94f3b706254" />

### Am√©liorations possibles

Ajouter plusieurs PDF (multi-corpus RAG)

Am√©liorer l‚ÄôUI Streamlit

Support des images / tableaux

Passage √† un LLM plus puissant (ex : Llama 3.1 8B)

üë§ Auteur

Projet r√©alis√© par Djebril Laouedj
√âtudiant en 5√®me ann√©e en Big Data & IA ‚Äì ECE Paris.
