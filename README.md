## ğŸ§  Mini RAG

Ce projet prÃ©sente un systÃ¨me RAG (Retrieval-Augmented Generation) complet permettant d'interroger un document PDF via un chatbot intelligent.
Il combine LangChain, FAISS, BGE embeddings, reranking et une gÃ©nÃ©ration locale via Ollama.

### ğŸš€ FonctionnalitÃ©s

- Chargement automatique dâ€™un PDF (ex : Syntec Conseil â€“ MÃ©tiers de la Data)

- DÃ©coupage intelligent du texte en chunks

- Vectorisation avec le modÃ¨le BAAI/bge-m3

- Recherche sÃ©mantique via FAISS

- Reranking des passages avec BAAI/bge-reranker-v2-m3

- GÃ©nÃ©ration locale avec mistral:instruct (Ollama)

- Interface Streamlit simple pour poser des questions

- Pipeline optimisÃ© : MMR + Reranker + Prompt anti-hallucination

### Structure du projet :

MiniRAG/

â”‚â”€â”€ My_Sreamlit_app.py           # Interface Streamlit

â”‚â”€â”€ MiniRAG_Djebril_LAOUEDJ.ipynb # Notebook du projet

â”‚â”€â”€ requirements.txt             # Versions exactes testÃ©es

â”‚â”€â”€ README.md                    # Documentation

â”‚â”€â”€ .gitignore                   # Fichiers ignorÃ©s

â””â”€â”€ data/                        # (Optionnel) PDF d'exemple

### ğŸ“˜ Exemple de pipeline RAG

Voici la logique gÃ©nÃ©rale du projet :

- Charger le PDF

- Chunking du texte

- GÃ©nÃ©ration des embeddings (BGE-M3)

- Indexation FAISS

- Retrieval (MMR)

- Reranking (BGE-Reranker)

- GÃ©nÃ©ration via Mistral:instruct (Ollama)

### âš™ï¸ Installation

1ï¸âƒ£ Cloner le projet

git clone https://github.com/djbrl-laouedj/MiniRAG.git
cd MiniRAG

2ï¸âƒ£ Installer les dÃ©pendances

âš ï¸ Les versions sont exactes pour Ã©viter les conflits LangChain. Merci d'utiliser requirements.txt.

pip install -r requirements.txt

3ï¸âƒ£ Installer Ollama (si pas dÃ©jÃ  installÃ©)

â¡ï¸ https://ollama.com/download

Puis tÃ©lÃ©charger le modÃ¨le utilisÃ© :

ollama pull mistral:instruct

â–¶ï¸ Lancer l'application Streamlit

streamlit run app.py

Lâ€™interface sâ€™ouvre automatiquement dans le navigateur.

### Sur Google Colab :

TÃ©lÃ©chargez tous les fichiers.

ğŸ”§ Configuration ngrok

Si vous voulez exposer votre interface Streamlit :

CrÃ©er un compte : https://ngrok.com

RÃ©cupÃ¨rez votre clÃ© : https://dashboard.ngrok.com/get-started/your-authtoken

Puis ajoutez-cela Ã  la fin d evotre code :

from pyngrok import ngrok
ngrok.set_auth_token("<VotreClÃ©>")

!streamlit run My_Sreamlit_app.py &>/dev/null &

public_url = ngrok.connect(8501)
public_url

Et ce code pour relancer le streamlit :

// stop tous les tunnels ngrok pour repartir propre
from pyngrok import ngrok
try:
    ngrok.kill()
except:
    pass

### VoilÃ  pour finir un mini user-guide :

1. Uploader un PDF :

<img width="230" height="174" alt="image" src="https://github.com/user-attachments/assets/68d63482-dab0-4fd7-9da6-eb2c532127a1" />

2. Puis lancer la pipeline :

<img width="228" height="73" alt="image" src="https://github.com/user-attachments/assets/1283f0f6-4c97-4e82-8e59-24a076c70c15" />

3. Une fois fini, poser une question (exemple : Quelle est le rÃ´le d'un Data Engineer)

<img width="1599" height="809" alt="image" src="https://github.com/user-attachments/assets/ecf844b9-4725-402d-8bd0-124febe23a01" />

4. RÃ©sultat (les sources peuvent Ãªtre vues ou non)

<img width="1550" height="694" alt="image" src="https://github.com/user-attachments/assets/ba5e04cc-467a-40ae-8506-52ae47a5799f" />

5. Malheureusement, si la question posÃ© ne peut pas Ãªtre rÃ©pondu (par manque d'informations dans les donnÃ©es jointes), le chatbot retournera cela :

<img width="1028" height="154" alt="image" src="https://github.com/user-attachments/assets/fb838479-3c1a-4307-97ba-a94f3b706254" />

ğŸ’¡ AmÃ©liorations possibles

Ajouter plusieurs PDF (multi-corpus RAG)

AmÃ©liorer lâ€™UI Streamlit

Support des images / tableaux

Passage Ã  un LLM plus puissant (ex : Llama 3.1 8B)

ğŸ‘¤ Auteur

Projet rÃ©alisÃ© par Djebril Laouedj
Ã‰tudiant en 5Ã¨me annÃ©e en Big Data & IA â€“ ECE Paris.
