{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR_CCVKPGgQR"
      },
      "source": [
        "# ***Mini RAG - Glossaire des principaux métiers de la Data***\n",
        "\n",
        "## *DJEBRIL LAOUEDJ*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9EgBLK-GsZQ"
      },
      "source": [
        "## **1. Ingestion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0GV_wgZMhmT"
      },
      "source": [
        "* *charger (le coprus)*\n",
        "\n",
        "* *nettoyer (préparation des données)*\n",
        "\n",
        "* *découper (en chunks)*\n",
        "\n",
        "* *sauvegarder (Un chunk par ligne)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCj3TzbXNkbs"
      },
      "source": [
        "## *1.1 - Installation de Langchain*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === INSTALLATION AUTOMATIQUE ===\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "import langchain, langchain_core, langchain_community, langchain_experimental, pydantic\n",
        "print(\"Versions installées :\")\n",
        "print(\"langchain\", langchain.__version__)\n",
        "print(\"core     \", langchain_core.__version__)\n",
        "print(\"community\", langchain_community.__version__)\n",
        "print(\"pydantic \", pydantic.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BORWYcXvOX0p",
        "outputId": "f5a02176-a833-4f75-bd42-11fc4054778e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.3.79 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mVersions installées :\n",
            "langchain 0.3.27\n",
            "core      0.3.79\n",
            "community 0.3.31\n",
            "pydantic  2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "except Exception:\n",
        "    from langchain.retrievers import ContextualCompressionRetriever  # fallback\n",
        "\n",
        "CrossEncoderReranker = None\n",
        "try:\n",
        "    from langchain_community.document_compressors import CrossEncoderReranker\n",
        "except Exception:\n",
        "    try:\n",
        "        from langchain_experimental.retrievers.document_compressors import CrossEncoderReranker\n",
        "    except Exception:\n",
        "        try:\n",
        "            from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "        except Exception as e:\n",
        "            raise ImportError(\n",
        "                \"Impossible d'importer CrossEncoderReranker dans cette stack. \"\n",
        "                \"Vérifie bien les versions installées ci-dessus.\"\n",
        "            ) from e\n",
        "\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "WvwijLQ_aP1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4djUvT6NvYw"
      },
      "source": [
        "## *1.2 - Chargement du corpus*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RviWO-4Gvcq"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf\")\n",
        "# if you use Google colab\n",
        "# loader = PyPDFLoader(\"/content/Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aZNp6yDN42z"
      },
      "source": [
        "## *1.3 - Découpage du texte en chunks*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdU7IpBzN_Jp"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn-HEjeOOI3e"
      },
      "source": [
        "## *1.4 - Affichage d'un aperçu*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9F3DlSwOIZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29a539b-e469-4570-c274-636d8816276e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de chunks : 22\n",
            "Exemple du quatrième chunk :\n",
            "Le Chief Analytics Officer exploite des outils informatiques, \n",
            "techniques et utilise des méthodes statistiques (y compris data \n",
            "science) pour permettre d’organiser, synthétiser et traduire \n",
            "efficacement les données.\n",
            "Il repère, parmi toutes les informations à disposition de \n",
            "l’entreprise, quelles sont les plus importantes / pertinentes à \n",
            "extraire pour des prises de décisions optimales, en s’appuyant \n",
            "sur une méthodologie objective basée sur les statistiques. Le cas \n",
            "échéant, il s’assure que les informations recueillies en interne ou \n",
            "en externe sont fiables, cohérentes, et prêtes à être analysées.\n",
            "Il peut aussi piloter l’industrialisation du procédé pour les \n",
            "données les plus intéressantes. \n"
          ]
        }
      ],
      "source": [
        "print(\"Nombre de chunks :\", len(chunks))\n",
        "print(\"Exemple du quatrième chunk :\")\n",
        "print(chunks[3].page_content[:700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlI48EXnG7_P"
      },
      "source": [
        "## **2. Embeddings & Index + Retriever**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeppxs6Q3UL"
      },
      "source": [
        "## *2.1 - Choix du modèle d'embedding*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neEXgOXeXvrN"
      },
      "source": [
        "*Modèles candidats :*\n",
        "\n",
        "- *bge-m3*\n",
        "\n",
        "- *multilingual-e5-small*\n",
        "\n",
        "- *paraphrase-multilingual-MiniLM-L12-v2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO3rTZyiiHgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3120935e-7b9c-4a38-c098-0dc6ad2afcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "modele_embedding = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-m3\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdqRBTQEeBDP"
      },
      "source": [
        "## *2.2 - Vector store (FAISS)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4L52A-3hQNn"
      },
      "source": [
        "*Index qui fait du kNN pour retrouver les chunks proches d’une question.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwA7DQtzdtpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594dec54-9e0e-49c3-ef43-1b053e788281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index FAISS prêt\n"
          ]
        }
      ],
      "source": [
        "vectorstore = FAISS.from_documents(chunks, modele_embedding)\n",
        "print(\"Index FAISS prêt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWDvG46jkZ3E"
      },
      "source": [
        "## *2.3 - Test*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIFpYQvCQSzk"
      },
      "source": [
        "*Plus le score est faible → plus le chunk est proche de la requête car sous FAISS, c’est une distance L2, donc plus petit = mieux.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkzcfcvdkdq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61bc8b6a-b39e-43b7-ec5a-b91ca0cfc480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1]  Score: 0.5325000286102295\n",
            "Référent\n",
            "Le Data Engineer (Ingénieur Data) développe l’infrastructure \n",
            "définie par/avec le Data Architect. Il construit les solutions \n",
            "techniques robustes (via des tests de robustesse) et fiables. Il en \n",
            "assure la maintenance et les évolutions conformément à l’état \n",
            "et des contraintes de sécurité.\n",
            "Il réalise l’intégration des données de diverses natures qui \n",
            "proviennent de ces sources multiples, l\n",
            "\n",
            "[2]  Score: 0.6524999737739563\n",
            "technologies \n",
            "de pointe\n",
            "Le Data Scientist traite, analyse et valorise les données d’une\n",
            "entreprise afin de définir la meilleure stratégie de développement : \n",
            "stratégie marketing et commerciale, amélioration des perfor -\n",
            "mances et de la rentabilité, prospective…\n",
            "Cumulant la connaissance des outils mathématiques / statistiques  \n",
            "et informatiques, il est capable de les coder (R, Python), \n",
            "de produire\n",
            "\n",
            "[3]  Score: 0.6944000124931335\n",
            "Spark SQL) et Hadoop pour la partie Big Data, le Cloud, \n",
            "les méthodes DevOps et CRISP.\n",
            "• Bac+5 obligatoire\n",
            "• École d’Ingénieur spécialisée\n",
            "Formations\n",
            "•  Mastère Pro Expert en Ingénierie Informatique, \n",
            "option Business Intelligence et Big Data (Lyon) \n",
            "• Ecole 42, Jedha Bootcamp….\n",
            "•  Master spécialisé Big Data (Télécom ParisTech, EM Grenoble, \n",
            "Essec...) \n",
            "•  Master Informatique, spécialité Exploration\n"
          ]
        }
      ],
      "source": [
        "q = \"Quelles sont les missions d’un Data Engineer ?\"\n",
        "# on récupère les k chunks les plus proches (par similarité cosinus)\n",
        "# avec leurs scores\n",
        "docs_with_scores = vectorstore.similarity_search_with_score(q, k=3)\n",
        "\n",
        "for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
        "    print(f\"\\n[{i}]  Score: {round(score, 4)}\")\n",
        "    print(doc.page_content[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eDUirDNTg2"
      },
      "source": [
        "## *2.4 - Petit test MMR (Maximal Marginal Relevance est plus pertinent)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCax_dItNWFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19bf94c2-2924-4231-e362-65f81daf6c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[MMR 1] Référent\n",
            "Le Data Engineer (Ingénieur Data) développe l’infrastructure \n",
            "définie par/avec le Data Architect. Il construit les solutions \n",
            "techniques robustes (via des tests de robustesse) et fiables. Il en \n",
            "assure la maintenance et les évolutions conformément à l’état \n",
            "et des contraintes de sécurité.\n",
            "Il réalise l’intégration des données de diverses na...\n",
            "\n",
            "[MMR 2] Glossaire\n",
            "des principaux \n",
            "métiers de \n",
            "la data...\n",
            "\n",
            "[MMR 3] de la conformité et de l’éthique. \n",
            "Le DPO est chargé de veiller la conformité au règlement, de \n",
            "définir les rôles et responsabilités de chacun, d’établir une \n",
            "cartographie des traitements et flux de données, de tenir le \n",
            "registre des traitements et de piloter la gestion des incidents \n",
            "de sécurité (y compris avec les sous-traitants).\n",
            "En France, ce r...\n",
            "\n",
            "[MMR 4] donne \n",
            "du sens\n",
            "Performance\n",
            "L’artiste \n",
            "de la donnée, \n",
            "le peintre du tableau \n",
            "et qui excelle dans \n",
            "la description de \n",
            "son oeuvre...\n",
            "le chef \n",
            "de production \n",
            "de l’équipe\n",
            "DataOps Engineer\n",
            "Le Data Visualisation Consultant est le storyteller de l’entreprise. \n",
            "Il est capable d’exploiter les données de l’entreprise, de les \n",
            "contextualiser et de proposer des...\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 4, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
        ")\n",
        "\n",
        "docs_mmr = retriever.invoke(q)\n",
        "for i, d in enumerate(docs_mmr, 1):\n",
        "    print(f\"\\n[MMR {i}] {d.page_content[:350]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuSm4QkLHFd1"
      },
      "source": [
        "## **3. Reranker**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpq9D3ufWNFE"
      },
      "source": [
        "*On sélectionne un pool de 20 passages, ensuite avec MMR, on sélectionne 8 passages diversifiés dans ce pool.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3jOSqXAHHMt"
      },
      "outputs": [],
      "source": [
        "base_ret = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8, \"fetch_k\": 20, \"lambda_mult\": 0.5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWw4tVzUWiwD"
      },
      "source": [
        "*On crée un compresseur/reranker basé sur le cross-encoder bge-reranker-v2-m3 de BAAI pour Langchain.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th2TxbLsT-4U"
      },
      "outputs": [],
      "source": [
        "ce = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\", model_kwargs={\"device\": \"cuda\"})\n",
        "# top_n = combien de passages garder après rerank (ex: 4)\n",
        "compressor = CrossEncoderReranker(model=ce, top_n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_cFxFj5ZHt6"
      },
      "source": [
        "**Combiner retriever de base et reranker pour avoir ce résultat :**\n",
        "\n",
        "*Quand on lui demande des docs, on obtient directement les meilleurs après reranking.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcwyrGVEWETE"
      },
      "outputs": [],
      "source": [
        "rerank_ret = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_ret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUixmOHqZaj6"
      },
      "source": [
        "*Retourne la liste finale de passages les plus pertinents (déjà triés et filtrés) à donner au LLM.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvuN3d_wWFhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200196eb-a8d3-4b30-9578-51c34bc94fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1625506513.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs_for_llm = rerank_ret.get_relevant_documents(q)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Rerank 1] source=/content/Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf page=4\n",
            "Référent\n",
            "Le Data Engineer (Ingénieur Data) développe l’infrastructure \n",
            "définie par/avec le Data Architect. Il construit les solutions \n",
            "techniques robustes (via des tests de robustesse) et fiables. Il en \n",
            "assure la maintenance et les évolutions conformément à l’état \n",
            "et des contraintes de sécurité.\n",
            "Il réalise l’intégration des données de diverses na\n",
            "\n",
            "[Rerank 2] source=/content/Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf page=5\n",
            "technologies \n",
            "de pointe\n",
            "Le Data Scientist traite, analyse et valorise les données d’une\n",
            "entreprise afin de définir la meilleure stratégie de développement : \n",
            "stratégie marketing et commerciale, amélioration des perfor -\n",
            "mances et de la rentabilité, prospective…\n",
            "Cumulant la connaissance des outils mathématiques / statistiques  \n",
            "et informatiques, il \n",
            "\n",
            "[Rerank 3] source=/content/Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf page=6\n",
            "donne \n",
            "du sens\n",
            "Performance\n",
            "L’artiste \n",
            "de la donnée, \n",
            "le peintre du tableau \n",
            "et qui excelle dans \n",
            "la description de \n",
            "son oeuvre...\n",
            "le chef \n",
            "de production \n",
            "de l’équipe\n",
            "DataOps Engineer\n",
            "Le Data Visualisation Consultant est le storyteller de l’entreprise. \n",
            "Il est capable d’exploiter les données de l’entreprise, de les \n",
            "contextualiser et de proposer des\n",
            "\n",
            "[Rerank 4] source=/content/Syntec-Conseil_Glossaire-des-principaux-métiers-de-la-Data.pdf page=5\n",
            "capacité à identifier de manière plus précise et plus rapide des \n",
            "enjeux business et d’efficience opérationnelle\n",
            "•  Tirer profit des technologies de pointe pour obtenir une meilleure \n",
            "analyse des données et concevoir des modèles (prédictifs) \n",
            "•  Participer à l’industrialisation de ces modèles \n",
            "•  Combiner des méthodes d’analyse de données structuré\n"
          ]
        }
      ],
      "source": [
        "q = \"Quelles sont les missions d’un Data Engineer ?\"\n",
        "docs_for_llm = rerank_ret.get_relevant_documents(q)\n",
        "\n",
        "for i, d in enumerate(docs_for_llm, 1):\n",
        "    print(f\"\\n[Rerank {i}] source={d.metadata.get('source')} page={d.metadata.get('page')}\")\n",
        "    print(d.page_content[:350])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FputNILHJfn"
      },
      "source": [
        "## **4. LLM + Prompt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTxRmzSAjwh_"
      },
      "source": [
        "## *4.1 - Installer et lancer Ollama dans Colab*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFshvvtFjraF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443ed266-5fc7-4a1c-d9b6-9159d121e2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!nohup ollama serve > /dev/null 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrlxF1l7hnvT"
      },
      "source": [
        "## *4.2 - Téléchargement du modèle*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVeHJiGfhmti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cdfa07-ad16-4023-cf54-a3dad8cf1191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: ollama server not responding - could not connect to ollama server, run 'ollama serve' to start it\n"
          ]
        }
      ],
      "source": [
        "# Choix parmi ces modèles : mistral:instruct / phi3:mini / llama3.2:3b-instruct\n",
        "MODEL_NAME = \"mistral:instruct\"\n",
        "!ollama pull {MODEL_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB5WeYMOh1Ju"
      },
      "source": [
        "## *4.3 - Test rapide pour vérifier qu’Ollama répond*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Si cela ne marche pas, il faut relancer la 4.1 et la 4.2)"
      ],
      "metadata": {
        "id": "MX9JiXrmadjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAUFdLCEkIC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8011779-7a31-4437-ca5b-a3ea51d64e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bonjour, c'est moi ! Comment ça va ? (Hello there, it's me! How are you?)\n"
          ]
        }
      ],
      "source": [
        "import requests, json\n",
        "\n",
        "def ollama_chat(model, prompt):\n",
        "    url = \"http://127.0.0.1:11434/api/generate\"\n",
        "    data = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
        "    r = requests.post(url, json=data, timeout=120)\n",
        "    r.raise_for_status()\n",
        "    return r.json()[\"response\"]\n",
        "\n",
        "print(ollama_chat(MODEL_NAME, \"Dis bonjour en une phrase.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAut1wHukST6"
      },
      "source": [
        "## *4.4 - Intégration LangChain (LLM local Ollama)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmTA6ozKkUKz"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "llm = ChatOllama(model=MODEL_NAME, temperature=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClpG-7zele2O"
      },
      "source": [
        "## *4.5 - Brancher le LLM Ollama sur le mini RAG existant*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6QKnbqFljjs"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Tu es un assistant des métiers de la Data. \"\n",
        "    \"Réponds UNIQUEMENT à partir du CONTEXTE fourni. \"\n",
        "    \"Si l'information n'y est pas, dis : 'Je ne sais pas sur la base du document.'\\n\\n\"\n",
        "    \"[CONTEXTE]\\n{context}\\n\\n[QUESTION]\\n{question}\\n\\n\"\n",
        "    \"Réponse concise en français :\"\n",
        ")\n",
        "\n",
        "qa_local = RetrievalQA.from_chain_type(\n",
        "    llm=llm,                          # Ollama (local)\n",
        "    retriever=rerank_ret,             # Retriever MMR + BGE-reranker\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZfFjLLah_m7"
      },
      "source": [
        "# 5. Test du RAG local"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *5.1 - Test Simple*"
      ],
      "metadata": {
        "id": "-q57JrAgY0VZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFUoHfN3iBZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccb412b-a90e-4f95-ec4f-1df50f8c64dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réponse :\n",
            "  Les missions d'un Data Engineer consistent à développer l'infrastructure définie par le Data Architect, construire des solutions techniques robustes et fiables, assurer leur maintenance et évolutions conformément aux contraintes de sécurité. Il réalise l'intégration des données de diverses natures, les supervise et vérifie la qualité des données. En production, il assure le suivi et le monitoring des flux/interfaces de données. Il s'assure aussi que ses travaux sont suffisamment documentés.\n"
          ]
        }
      ],
      "source": [
        "question = \"Quelles sont les missions d’un Data Engineer ?\"\n",
        "res = qa_local.invoke({\"query\": question})\n",
        "\n",
        "print(\"Réponse :\\n\", res[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *5.2 - Test Interactif (Style ChatBot)*"
      ],
      "metadata": {
        "id": "UTTYqI1NY3XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_utilisateur = input(\"Entrez une question : \")\n",
        "res = qa_local.invoke({\"query\": question_utilisateur})\n",
        "\n",
        "print(\"Réponse :\\n\", res[\"result\"])"
      ],
      "metadata": {
        "id": "0EZJBE8CY-iA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578a1cfd-a233-43e9-f84f-0071cfcdbfa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrez une question : Quel est le rôle d'un Data Analyst ?\n",
            "Réponse :\n",
            "  Le rôle d'un Data Analyst consiste à explorer et analyser des données pour identifier des tendances, déterminer les relations entre les données et fournir des informations utiles pour prendre des décisions optimales. Il travaille souvent en lien avec le Data Scientist et le Data Engineer.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}