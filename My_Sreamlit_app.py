# -*- coding: utf-8 -*-
"""My_Sreamlit_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eX1WCHf0yPmqzi_Tx_UZ5fLy-Tia4QYb
"""

import os, io, tempfile, hashlib
import streamlit as st

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

LLM_MODEL = "mistral:instruct"  # Ollama
EMB_MODEL = "BAAI/bge-m3"
USE_RERANKER = True
RERANK_MODEL = "BAAI/bge-reranker-v2-m3"

def auto_device():
    try:
        import torch
        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"

def tmp_path_from_upload(upload):
    tmpdir = tempfile.mkdtemp()
    path = os.path.join(tmpdir, upload.name)
    with open(path, "wb") as f:
        f.write(upload.getbuffer())
    return path

def hash_files(paths):
    h = hashlib.sha256()
    for p in sorted(paths):
        h.update(p.encode())
        try:
            h.update(str(os.path.getsize(p)).encode())
        except Exception:
            pass
    return h.hexdigest()

# UI

st.set_page_config(page_title="Mini RAG ‚Äî DJEBRIL LAOUEDJ", page_icon="üìö", layout="wide")
st.title("üìö Mini RAG ‚Äî djbrl-laouedj")

with st.sidebar:
    st.subheader("‚öôÔ∏è Configuration")
    uploads = st.file_uploader("Charge un ou plusieurs PDF", type=["pdf"], accept_multiple_files=True)

    with st.expander("üîß Avanc√© (chunking, MMR, device)"):
        device_choice = st.selectbox("Device", ["auto", "cuda", "cpu"], index=0, help="`auto` = CUDA si dispo, sinon CPU")
        chunk_size = st.slider("Chunk size", 500, 2000, 1000, step=50)
        chunk_overlap = st.slider("Chunk overlap", 0, 400, 150, step=10)
        mmr_k = st.slider("MMR k (s√©lection finale avant rerank)", 2, 12, 8)
        mmr_fetch = st.slider("MMR fetch_k (pool initial)", 8, 64, 20, step=2)
        mmr_lambda = st.slider("MMR lambda (pertinence‚Üîdiversit√©)", 0.0, 1.0, 0.5, step=0.1)
        top_n = st.slider("Passages envoy√©s au LLM (apr√®s rerank)", 2, 6, 4)

    build_btn = st.button("üì• Construire / Recharger l‚Äôindex")

if "qa" not in st.session_state: st.session_state.qa = None
if "retriever" not in st.session_state: st.session_state.retriever = None
if "history" not in st.session_state: st.session_state.history = []

# Pipeline (√† partir du notebook d√©j√† fait)

@st.cache_resource(show_spinner=True)
def build_pipeline(paths, device, chunk_size, chunk_overlap, mmr_k, mmr_fetch, mmr_lambda, top_n):
    # 0) Hash corpus pour persistance
    corpus_id = hash_files(paths)
    index_dir = f"./faiss_{corpus_id}_{EMB_MODEL.replace('/','-')}"

    # 1) Load & split (si on doit re-builder)
    need_build = not os.path.isdir(index_dir)
    if need_build:
        docs_all = []
        for p in paths:
            loader = PyPDFLoader(p)
            docs = loader.load()
            for d in docs:
                if "source" in d.metadata:
                    d.metadata["source"] = os.path.basename(d.metadata["source"])
            docs_all.extend(docs)
        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        chunks = splitter.split_documents(docs_all)
    else:
        chunks = None  # on va juste recharger l'index

    # 2) Embeddings (BGE-m3)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMB_MODEL,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True, "batch_size": 16},
    )

    # 3) FAISS (load or build)
    if need_build:
        vs = FAISS.from_documents(chunks, embeddings)
        vs.save_local(index_dir)
    else:
        vs = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)

    # 4) Retriever (MMR)
    base_ret = vs.as_retriever(
        search_type="mmr",
        search_kwargs={"k": mmr_k, "fetch_k": mmr_fetch, "lambda_mult": mmr_lambda}
    )

    # 5) Reranker
    if USE_RERANKER:
        try:
            ce = HuggingFaceCrossEncoder(model_name=RERANK_MODEL, model_kwargs={"device": device})
        except RuntimeError:
            # fallback CPU si VRAM satur√©e
            ce = HuggingFaceCrossEncoder(model_name=RERANK_MODEL, model_kwargs={"device": "cpu"})
        compressor = CrossEncoderReranker(model=ce, top_n=top_n)
        retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_ret)
    else:
        retriever = base_ret

    # 6) LLM (Ollama)
    llm = ChatOllama(model=LLM_MODEL, temperature=0.2)

    # 7) Prompt + QA Chain
    prompt = PromptTemplate.from_template(
        "Tu es un assistant d‚Äôorientation sur les m√©tiers de la Data.\n"
        "R√©ponds UNIQUEMENT √† partir du CONTEXTE fourni. Si l'information n'y est pas, dis : "
        "\"Je ne sais pas sur la base du document.\"\n\n"
        "[CONTEXTE]\n{context}\n\n[QUESTION]\n{question}\n\n"
        "R√©ponse concise en fran√ßais, structur√©e (puces si utile) :"
    )

    qa = RetrievalQA.from_chain_type(
        llm=llm, retriever=retriever, chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}, return_source_documents=True
    )
    return qa, retriever



# Build pipeline / Rebuild

def collect_paths():
    return [tmp_path_from_upload(u) for u in (uploads or [])]

if build_btn:
    paths = collect_paths()
    if not paths:
        st.sidebar.error("Ajoute au moins un PDF (upload).")
    else:
        dev = auto_device() if device_choice == "auto" else device_choice
        with st.status("Construction de l‚Äôindex‚Ä¶", expanded=False):
            qa, retriever = build_pipeline(
                paths=paths, device=dev,
                chunk_size=chunk_size, chunk_overlap=chunk_overlap,
                mmr_k=mmr_k, mmr_fetch=mmr_fetch, mmr_lambda=mmr_lambda, top_n=top_n
            )
        st.session_state.qa = qa
        st.session_state.retriever = retriever
        st.success(f"Pipeline pr√™t ‚úÖ  |  {len(paths)} PDF")

# Zone Q/A (chat)

st.subheader("üí¨ Pose ta question")
for role, msg in st.session_state.history:
    with st.chat_message(role):
        st.markdown(msg)

prompt_user = st.chat_input("Exemple : Quelles sont les missions d‚Äôun Data Engineer ?")
if prompt_user:
    if not st.session_state.qa:
        st.warning("Construis d‚Äôabord l‚Äôindex dans la barre lat√©rale.")
    else:
        st.session_state.history.append(("user", prompt_user))
        with st.chat_message("user"):
            st.markdown(prompt_user)

        with st.chat_message("assistant"):
            with st.spinner("Je r√©fl√©chis‚Ä¶"):
                out = st.session_state.qa.invoke({"query": prompt_user})
                answer = out["result"]
                st.session_state.history.append(("assistant", answer))
                st.markdown(answer)

                if st.toggle("Afficher les sources & passages", value=True):
                    seen = set()
                    st.markdown("**üìö Sources**")
                    for d in out["source_documents"]:
                        src = d.metadata.get("source", "PDF")
                        page = d.metadata.get("page")
                        key = (src, page)
                        if key in seen:
                            continue
                        seen.add(key)
                        with st.expander(f"{src} ‚Äî page {page}"):
                            st.caption((d.page_content[:700] + "‚Ä¶").replace("\n", " "))